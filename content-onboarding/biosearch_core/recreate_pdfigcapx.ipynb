{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import listdir\n",
    "\n",
    "content_base_folder = Path('/media/cumulus/biosearch/cord19/to_import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders = content_base_folder.iterdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move folders without pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosearch_core.db_importer.importer import Validator\n",
    "validator = Validator()\n",
    "# all have pdfs and one pdf\n",
    "\n",
    "\n",
    "folders = listdir(content_base_folder)\n",
    "for folder in folders:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_name(folder: Path) -> str:\n",
    "    pdfs = [x for x in listdir(folder) if x.endswith(\".pdf\")]\n",
    "    return pdfs[0][:-4]\n",
    "\n",
    "def folder_with_data_exists(folder: Path) -> bool:\n",
    "    pdf_name = find_pdf_name(folder)\n",
    "    return (folder / pdf_name).exists()\n",
    "\n",
    "def find_candidates_to_extraction():\n",
    "    candidates_extraction = []\n",
    "    candidates_no_pdfs = []\n",
    "    folders = listdir(content_base_folder)\n",
    "    for folder in folders:\n",
    "        folder_path = content_base_folder / folder\n",
    "        try:\n",
    "            if not folder_with_data_exists(folder_path):\n",
    "                candidates_extraction.append(folder_path)\n",
    "        except IndexError:\n",
    "            candidates_no_pdfs.append(folder_path)\n",
    "    return candidates_extraction, candidates_no_pdfs\n",
    "\n",
    "def metadata_exists(folder: Path) -> bool:\n",
    "    pdf_name = find_pdf_name(folder)[:-4]\n",
    "    metadata_path = folder / pdf_name / f\"{pdf_name}.json\"\n",
    "    return metadata_path.exists()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_to_extraction, candidates_no_pdfs  = find_candidates_to_extraction()\n",
    "# every folder has a folder for the pdf and only one pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    no_metadata = []\n",
    "    folder_path = content_base_folder / folder\n",
    "    if not metadata_exists(folder_path):        \n",
    "        no_metadata.append(folder)\n",
    "print(len(no_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\"\"\"Fixing common Unicode mistakes with Python â€” after they’ve been made\"\n",
    "#Adapted for use in Python 3.x from:\n",
    "http://blog.luminoso.com/2012/08/20/fix-unicode-mistakes-with-python/ \n",
    "\"\"\"\n",
    "def fix_bad_unicode(text):\n",
    "    u\"\"\"\n",
    "    Something you will find all over the place, in real-world text, is text\n",
    "    that's mistakenly encoded as utf-8, decoded in some ugly format like\n",
    "    latin-1 or even Windows codepage 1252, and encoded as utf-8 again.\n",
    "    This causes your perfectly good Unicode-aware code to end up with garbage\n",
    "    text because someone else (or maybe \"someone else\") made a mistake.\n",
    "    This function looks for the evidence of that having happened and fixes it.\n",
    "    It determines whether it should replace nonsense sequences of single-byte\n",
    "    characters that were really meant to be UTF-8 characters, and if so, turns\n",
    "    them into the correctly-encoded Unicode character that they were meant to\n",
    "    represent.\n",
    "    The input to the function must be Unicode. It's not going to try to\n",
    "    auto-decode bytes for you -- then it would just create the problems it's\n",
    "    supposed to fix.\n",
    "        >>> print fix_bad_unicode(u'Ãºnico')\n",
    "        único\n",
    "        >>> print fix_bad_unicode(u'This text is fine already :þ')\n",
    "        This text is fine already :þ\n",
    "    Because these characters often come from Microsoft products, we allow\n",
    "    for the possibility that we get not just Unicode characters 128-255, but\n",
    "    also Windows's conflicting idea of what characters 128-160 are.\n",
    "        >>> print fix_bad_unicode(u'This â€” should be an em dash')\n",
    "        This — should be an em dash\n",
    "    We might have to deal with both Windows characters and raw control\n",
    "    characters at the same time, especially when dealing with characters like\n",
    "    \\x81 that have no mapping in Windows.\n",
    "        >>> print fix_bad_unicode(u'This text is sad .â\\x81”.')\n",
    "        This text is sad .⁔.\n",
    "    This function even fixes multiple levels of badness:\n",
    "        >>> wtf = u'\\xc3\\xa0\\xc2\\xb2\\xc2\\xa0_\\xc3\\xa0\\xc2\\xb2\\xc2\\xa0'\n",
    "        >>> print fix_bad_unicode(wtf)\n",
    "        ಠ_ಠ\n",
    "    However, it has safeguards against fixing sequences of letters and\n",
    "    punctuation that can occur in valid text:\n",
    "        >>> print fix_bad_unicode(u'not such a fan of Charlotte Brontë…”')\n",
    "        not such a fan of Charlotte Brontë…”\n",
    "    Cases of genuine ambiguity can sometimes be addressed by finding other\n",
    "    characters that are not double-encoding, and expecting the encoding to\n",
    "    be consistent:\n",
    "        >>> print fix_bad_unicode(u'AHÅ™, the new sofa from IKEA®')\n",
    "        AHÅ™, the new sofa from IKEA®\n",
    "    Finally, we handle the case where the text is in a single-byte encoding\n",
    "    that was intended as Windows-1252 all along but read as Latin-1:\n",
    "        >>> print fix_bad_unicode(u'This text was never Unicode at all\\x85')\n",
    "        This text was never Unicode at all…\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"This isn't even decoded into Unicode yet. \"\n",
    "                        \"Decode it first.\")\n",
    "    if len(text) == 0:\n",
    "        return text\n",
    "\n",
    "    maxord = max(ord(char) for char in text)\n",
    "    tried_fixing = []\n",
    "    if maxord < 128:\n",
    "        # Hooray! It's ASCII!\n",
    "        return text\n",
    "    else:\n",
    "        attempts = [(text, text_badness(text) + len(text))]\n",
    "        if maxord < 256:\n",
    "            tried_fixing = reinterpret_latin1_as_utf8(text)\n",
    "            tried_fixing2 = reinterpret_latin1_as_windows1252(text)\n",
    "            attempts.append((tried_fixing, text_cost(tried_fixing)))\n",
    "            attempts.append((tried_fixing2, text_cost(tried_fixing2)))\n",
    "        elif all(ord(char) in WINDOWS_1252_CODEPOINTS for char in text):\n",
    "            tried_fixing = reinterpret_windows1252_as_utf8(text)\n",
    "            attempts.append((tried_fixing, text_cost(tried_fixing)))\n",
    "        else:\n",
    "            # We can't imagine how this would be anything but valid text.\n",
    "            return text\n",
    "\n",
    "        # Sort the results by badness\n",
    "        attempts.sort(key=lambda x: x[1])\n",
    "        goodtext = attempts[0][0]\n",
    "        if goodtext == text:\n",
    "            return goodtext\n",
    "        else:\n",
    "            return fix_bad_unicode(goodtext)\n",
    "\n",
    "def reinterpret_latin1_as_utf8(wrongtext):\n",
    "    newbytes = wrongtext.encode('latin-1', 'replace')\n",
    "    return newbytes.decode('utf-8', 'replace')\n",
    "\n",
    "def fix_windows_1252_char(char):\n",
    "    if ord(char) in WINDOWS_1252_GREMLINS:\n",
    "        return char.encode('WINDOWS_1252')\n",
    "    return char.encode('latin-1', 'replace')\n",
    "\n",
    "def reinterpret_windows1252_as_utf8(wrongtext):\n",
    "    altered_bytes = [ fix_windows_1252_char(char) for char in wrongtext ]\n",
    "    return b''.join(altered_bytes).decode('utf-8', 'replace')\n",
    "\n",
    "def reinterpret_latin1_as_windows1252(wrongtext):\n",
    "    \"\"\"\n",
    "    Maybe this was always meant to be in a single-byte encoding, and it\n",
    "    makes the most sense in Windows-1252.\n",
    "    \"\"\"\n",
    "    return wrongtext.encode('latin-1').decode('WINDOWS_1252', 'replace')\n",
    "\n",
    "def text_badness(text):\n",
    "    u'''\n",
    "    Look for red flags that text is encoded incorrectly:\n",
    "    Obvious problems:\n",
    "    - The replacement character \\ufffd, indicating a decoding error\n",
    "    - Unassigned or private-use Unicode characters\n",
    "    Very weird things:\n",
    "    - Adjacent letters from two different scripts\n",
    "    - Letters in scripts that are very rarely used on computers (and\n",
    "      therefore, someone who is using them will probably get Unicode right)\n",
    "    - Improbable control characters, such as 0x81\n",
    "    Moderately weird things:\n",
    "    - Improbable single-byte characters, such as ƒ or ¬\n",
    "    - Letters in somewhat rare scripts\n",
    "    '''\n",
    "    assert isinstance(text, str)\n",
    "    errors = 0\n",
    "    very_weird_things = 0\n",
    "    weird_things = 0\n",
    "    prev_letter_script = None\n",
    "    unicodedata_name = unicodedata.name\n",
    "    unicodedata_category = unicodedata.category\n",
    "    for char in text:\n",
    "        index = ord(char)\n",
    "        if index < 256:\n",
    "            # Deal quickly with the first 256 characters.\n",
    "            weird_things += SINGLE_BYTE_WEIRDNESS[index]\n",
    "            if SINGLE_BYTE_LETTERS[index]:\n",
    "                prev_letter_script = 'latin'\n",
    "            else:\n",
    "                prev_letter_script = None\n",
    "        else:\n",
    "            category = unicodedata_category(char)\n",
    "            if category == 'Co':\n",
    "                # Unassigned or private use\n",
    "                errors += 1\n",
    "            elif index == 0xfffd:\n",
    "                # Replacement character\n",
    "                errors += 1\n",
    "            elif index in WINDOWS_1252_GREMLINS:\n",
    "                lowchar = char.encode('WINDOWS_1252').decode('latin-1')\n",
    "                weird_things += SINGLE_BYTE_WEIRDNESS[ord(lowchar)] - 0.5\n",
    "\n",
    "            if category[0] == 'L':\n",
    "                # It's a letter. What kind of letter? This is typically found\n",
    "                # in the first word of the letter's Unicode name.\n",
    "                name = unicodedata_name(char)\n",
    "                scriptname = name.split()[0]\n",
    "                freq, script = SCRIPT_TABLE.get(scriptname, (0, 'other'))\n",
    "                if prev_letter_script:\n",
    "                    if script != prev_letter_script:\n",
    "                        very_weird_things += 1\n",
    "                    if freq == 1:\n",
    "                        weird_things += 2\n",
    "                    elif freq == 0:\n",
    "                        very_weird_things += 1\n",
    "                prev_letter_script = script\n",
    "            else:\n",
    "                prev_letter_script = None\n",
    "\n",
    "    return 100 * errors + 10 * very_weird_things + weird_things\n",
    "\n",
    "def text_cost(text):\n",
    "    \"\"\"\n",
    "    Assign a cost function to the length plus weirdness of a text string.\n",
    "    \"\"\"\n",
    "    return text_badness(text) + len(text)\n",
    "\n",
    "#######################################################################\n",
    "# The rest of this file is esoteric info about characters, scripts, and their\n",
    "# frequencies.\n",
    "#\n",
    "# Start with an inventory of \"gremlins\", which are characters from all over\n",
    "# Unicode that Windows has instead assigned to the control characters\n",
    "# 0x80-0x9F. We might encounter them in their Unicode forms and have to figure\n",
    "# out what they were originally.\n",
    "\n",
    "WINDOWS_1252_GREMLINS = {\n",
    "    # adapted from http://effbot.org/zone/unicode-gremlins.htm\n",
    "    0x0152,  # LATIN CAPITAL LIGATURE OE\n",
    "    0x0153,  # LATIN SMALL LIGATURE OE\n",
    "    0x0160,  # LATIN CAPITAL LETTER S WITH CARON\n",
    "    0x0161,  # LATIN SMALL LETTER S WITH CARON\n",
    "    0x0178,  # LATIN CAPITAL LETTER Y WITH DIAERESIS\n",
    "    0x017E,  # LATIN SMALL LETTER Z WITH CARON\n",
    "    0x017D,  # LATIN CAPITAL LETTER Z WITH CARON\n",
    "    0x0192,  # LATIN SMALL LETTER F WITH HOOK\n",
    "    0x02C6,  # MODIFIER LETTER CIRCUMFLEX ACCENT\n",
    "    0x02DC,  # SMALL TILDE\n",
    "    0x2013,  # EN DASH\n",
    "    0x2014,  # EM DASH\n",
    "    0x201A,  # SINGLE LOW-9 QUOTATION MARK\n",
    "    0x201C,  # LEFT DOUBLE QUOTATION MARK\n",
    "    0x201D,  # RIGHT DOUBLE QUOTATION MARK\n",
    "    0x201E,  # DOUBLE LOW-9 QUOTATION MARK\n",
    "    0x2018,  # LEFT SINGLE QUOTATION MARK\n",
    "    0x2019,  # RIGHT SINGLE QUOTATION MARK\n",
    "    0x2020,  # DAGGER\n",
    "    0x2021,  # DOUBLE DAGGER\n",
    "    0x2022,  # BULLET\n",
    "    0x2026,  # HORIZONTAL ELLIPSIS\n",
    "    0x2030,  # PER MILLE SIGN\n",
    "    0x2039,  # SINGLE LEFT-POINTING ANGLE QUOTATION MARK\n",
    "    0x203A,  # SINGLE RIGHT-POINTING ANGLE QUOTATION MARK\n",
    "    0x20AC,  # EURO SIGN\n",
    "    0x2122,  # TRADE MARK SIGN\n",
    "}\n",
    "\n",
    "# a list of Unicode characters that might appear in Windows-1252 text\n",
    "WINDOWS_1252_CODEPOINTS = set(list(range(256)) + list(WINDOWS_1252_GREMLINS))\n",
    "\n",
    "# Rank the characters typically represented by a single byte -- that is, in\n",
    "# Latin-1 or Windows-1252 -- by how weird it would be to see them in running\n",
    "# text.\n",
    "#\n",
    "#   0 = not weird at all\n",
    "#   1 = rare punctuation or rare letter that someone could certainly\n",
    "#       have a good reason to use. All Windows-1252 gremlins are at least\n",
    "#       weirdness 1.\n",
    "#   2 = things that probably don't appear next to letters or other\n",
    "#       symbols, such as math or currency symbols\n",
    "#   3 = obscure symbols that nobody would go out of their way to use\n",
    "#       (includes symbols that were replaced in ISO-8859-15)\n",
    "#   4 = why would you use this?\n",
    "#   5 = unprintable control character\n",
    "#\n",
    "# The Portuguese letter Ã (0xc3) is marked as weird because it would usually\n",
    "# appear in the middle of a word in actual Portuguese, and meanwhile it\n",
    "# appears in the mis-encodings of many common characters.\n",
    "\n",
    "SINGLE_BYTE_WEIRDNESS = (\n",
    "#   0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f\n",
    "    5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5,  # 0x00\n",
    "    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,  # 0x10\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0x20\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0x30\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0x40\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0x50\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0x60\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5,  # 0x70\n",
    "    2, 5, 1, 4, 1, 1, 3, 3, 4, 3, 1, 1, 1, 5, 1, 5,  # 0x80\n",
    "    5, 1, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 5, 1, 1,  # 0x90\n",
    "    1, 0, 2, 2, 3, 2, 4, 2, 4, 2, 2, 0, 3, 1, 1, 4,  # 0xa0\n",
    "    2, 2, 3, 3, 4, 3, 3, 2, 4, 4, 4, 0, 3, 3, 3, 0,  # 0xb0\n",
    "    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0xc0\n",
    "    1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,  # 0xd0\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 0xe0\n",
    "    1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,  # 0xf0\n",
    ")\n",
    "\n",
    "# Pre-cache the Unicode data saying which of these first 256 characters are\n",
    "# letters. We'll need it often.\n",
    "SINGLE_BYTE_LETTERS = [\n",
    "    unicodedata.category(chr(i))[0] == 'L'\n",
    "    for i in range(256)\n",
    "]\n",
    "\n",
    "# A table telling us how to interpret the first word of a letter's Unicode\n",
    "# name. The number indicates how frequently we expect this script to be used\n",
    "# on computers. Many scripts not included here are assumed to have a frequency\n",
    "# of \"0\" -- if you're going to write in Linear B using Unicode, you're\n",
    "# probably aware enough of encoding issues to get it right.\n",
    "#\n",
    "# The lowercase name is a general category -- for example, Han characters and\n",
    "# Hiragana characters are very frequently adjacent in Japanese, so they all go\n",
    "# into category 'cjk'. Letters of different categories are assumed not to\n",
    "# appear next to each other often.\n",
    "SCRIPT_TABLE = {\n",
    "    'LATIN': (3, 'latin'),\n",
    "    'CJK': (2, 'cjk'),\n",
    "    'ARABIC': (2, 'arabic'),\n",
    "    'CYRILLIC': (2, 'cyrillic'),\n",
    "    'GREEK': (2, 'greek'),\n",
    "    'HEBREW': (2, 'hebrew'),\n",
    "    'KATAKANA': (2, 'cjk'),\n",
    "    'HIRAGANA': (2, 'cjk'),\n",
    "    'HIRAGANA-KATAKANA': (2, 'cjk'),\n",
    "    'HANGUL': (2, 'cjk'),\n",
    "    'DEVANAGARI': (2, 'devanagari'),\n",
    "    'THAI': (2, 'thai'),\n",
    "    'FULLWIDTH': (2, 'cjk'),\n",
    "    'MODIFIER': (2, None),\n",
    "    'HALFWIDTH': (1, 'cjk'),\n",
    "    'BENGALI': (1, 'bengali'),\n",
    "    'LAO': (1, 'lao'),\n",
    "    'KHMER': (1, 'khmer'),\n",
    "    'TELUGU': (1, 'telugu'),\n",
    "    'MALAYALAM': (1, 'malayalam'),\n",
    "    'SINHALA': (1, 'sinhala'),\n",
    "    'TAMIL': (1, 'tamil'),\n",
    "    'GEORGIAN': (1, 'georgian'),\n",
    "    'ARMENIAN': (1, 'armenian'),\n",
    "    'KANNADA': (1, 'kannada'),  # mostly used for looks of disapproval\n",
    "    'MASCULINE': (1, 'latin'),\n",
    "    'FEMININE': (1, 'latin')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def old_metadata_to_new(folder: Path):\n",
    "    pdf_name = find_pdf_name(folder)\n",
    "    metadata_path = folder / pdf_name / f\"{pdf_name}.json\"\n",
    "\n",
    "    with open(metadata_path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            data = json.load(f)\n",
    "    # print(data)\n",
    "    name = folder.name\n",
    "    xpdf_content_path = \"\"\n",
    "    width = -1\n",
    "    height = -1\n",
    "    new_pages = []\n",
    "\n",
    "    figures = data[\"figures\"]\n",
    "    tmp_fig_figures = defaultdict(list)\n",
    "    for figure in figures:\n",
    "        fixed_text = \"\"\n",
    "        for sentence in figure[\"caption_text\"]:\n",
    "            fixed_sentence = fix_bad_unicode(sentence)\n",
    "            if len(fixed_sentence) > 1:                  \n",
    "                last_char = fixed_sentence[len(fixed_sentence)-1]\n",
    "                fixed_sentence = fixed_sentence[:-1] if last_char == \"-\" else f\"{fixed_sentence} \"\n",
    "            fixed_text += fixed_sentence\n",
    "        \n",
    "        # fixed_text = fix_bad_unicode(\" \".join(figure[\"caption_text\"]))\n",
    "        page_number = figure[\"page\"]\n",
    "        fig_number_in_page = len(tmp_fig_figures[figure[\"page\"]]) + 1\n",
    "        id = f\"{pdf_name}/{page_number}_{fig_number_in_page}.jpg\"\n",
    "        width = figure[\"page_width\"]\n",
    "        height = figure[\"page_height\"]\n",
    "\n",
    "        tmp_fig_figures[figure[\"page\"]].append({\n",
    "            \"bbox\": figure[\"region_bb\"],\n",
    "            \"caption_bbox\": figure[\"caption_bb\"],\n",
    "            \"caption\": fixed_text,\n",
    "            \"name\": \"\",\n",
    "            \"id\": id,\n",
    "            \"page_width\": figure[\"page_width\"],\n",
    "            \"page_height\": figure[\"page_height\"]\n",
    "        })\n",
    "    for page_key in tmp_fig_figures.keys():\n",
    "        new_pages.append({\n",
    "            \"number\": page_key,\n",
    "            \"figures\": tmp_fig_figures[page_key]\n",
    "        })\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"xpdf_content_path\": \"\",\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"pages\": new_pages\n",
    "    }\n",
    "\n",
    "meta = old_metadata_to_new(content_base_folder / folders[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = []\n",
    "errors = []\n",
    "for folder in folders:\n",
    "    try:\n",
    "        folder_path = content_base_folder / folder\n",
    "        meta = old_metadata_to_new(folder_path)\n",
    "\n",
    "        new_meta_path = folder_path / f\"{folder_path.name}.json\"\n",
    "        with open(new_meta_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            json_object = json.dumps(meta, indent=4)\n",
    "            f_out.write(json_object)\n",
    "        metas.append(meta)\n",
    "    except FileNotFoundError:\n",
    "        errors.append(folder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PMC3878732', 'PMC5877055', 'PMC7070519']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PMC7155165', 'xpdf_content_path': '', 'width': 612, 'height': 782, 'pages': [{'number': 15, 'figures': [{'bbox': [50, 61, 654, 485], 'caption_bbox': [], 'caption': '', 'name': '', 'id': 'main/15_1.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 14, 'figures': [{'bbox': [51, 48, 654, 505], 'caption_bbox': [], 'caption': '', 'name': '', 'id': 'main/14_1.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 13, 'figures': [{'bbox': [50, 61, 654, 485], 'caption_bbox': [], 'caption': '', 'name': '', 'id': 'main/13_1.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 10, 'figures': [{'bbox': [73.0, 79.0, 461.0, 175.0], 'caption_bbox': [398.0, 289.0, 147.0, 78.0], 'caption': 'FIGURE 18.3 Assortment of carbon allotropes as regards their dimension. Reprinted with permission from N. Baig, M. Sajid, T.A. Saleh, Recent trends in nanomaterial-modified electrodes for electroanalytical applications, TrAC—Trends Anal. Chem. 111 (2019) 47À61. Available from: https://doi.org/10.1016/j. trac.2018.11.044 [103]. ', 'name': '', 'id': 'main/10_1.jpg', 'page_width': 612, 'page_height': 782}, {'bbox': [61, 289.0, 337.0, 313.0], 'caption_bbox': [398.0, 289.0, 147.0, 78.0], 'caption': 'FIGURE 18.3 Assortment of carbon allotropes as regards their dimension. Reprinted with permission from N. Baig, M. Sajid, T.A. Saleh, Recent trends in nanomaterial-modified electrodes for electroanalytical applications, TrAC—Trends Anal. Chem. 111 (2019) 47À61. Available from: https://doi.org/10.1016/j. trac.2018.11.044 [103]. ', 'name': '', 'id': 'main/10_2.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 9, 'figures': [{'bbox': [61, 405, 325.0, 310], 'caption_bbox': [386.0, 407.0, 157.0, 88.0], 'caption': 'FIGURE 18.1 The schematic diagram of target analytes, biorecognition elements, and detection of a typical nanobiosensor. Reprinted with permission from R. Shandilya, A. Bhargava, N. Bunkar, R. Tiwari, I.Y. Goryacheva, P.K. Mishra, Nanobiosensors: point-of-care approaches for cancer diagnostics, Biosens. Bioelectron. 130 (2019) 147À165. Available from: https://doi.org/ 10.1016/j.bios.2019.01.034 [95]. ', 'name': '', 'id': 'main/9_1.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 12, 'figures': [{'bbox': [50, 61, 654, 489], 'caption_bbox': [], 'caption': '', 'name': '', 'id': 'main/12_1.jpg', 'page_width': 612, 'page_height': 782}]}, {'number': 16, 'figures': [{'bbox': [50, 61, 654, 328], 'caption_bbox': [], 'caption': '', 'name': '', 'id': 'main/16_1.jpg', 'page_width': 612, 'page_height': 782}]}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"/media/cumulus/biosearch/cord19/to_import/PMC7155165/PMC7155165.json\",'r', encoding=\"utf-8\") as f_in:\n",
    "    data = json.load(f_in)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67ab03f1fe994222664e12fe4ba3f091b198cb5337ab8b92ec3312d3f3349829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
